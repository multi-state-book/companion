### Exercise 3.1 (\*)

*Show that, under the null hypothesis $H_0 : A_0(t) = A_1(t)$, the test statistic* $\int_0^t K(u)\left( d\widehat{A}_1(u)-d\widehat{A}_0(u)\right)$
*is a martingale (Section 3.2.2).*

The counting processes can be decomposed as $N_h(t)=\int_0^tY_h(u)\alpha_h(u)du + M_h(t), h=0,1$, so under $H_0$,
$$d\widehat{A}_h(u)=\frac{dN_h(u)}{Y_h(u)}=\alpha(u)du+\frac{dM_h(u)}{Y_h(u)},$$
and the test statistic reduces to the martingale integral
$$\int_0^tK(u)\left(\frac{dM_1(u)}{Y_1(u)}-\frac{dM_0(u)}{Y_0(u)}\right),$$
which is itself a martingale.

### Exercise 3.2 (\*)
*Show that, when evaluated at the true parameter vector* $\mathbf{\beta}_0$*, the Cox partial likelihood score*
$$
\int_0^t\sum_i \bigl(\mathbf{Z}_i-\frac{\sum_jY_j(u)\mathbf{Z}_j\exp(\mathbf{\beta}_0^{\sf T}\mathbf{Z}_j)}{\sum_jY_j(u)\exp(\mathbf{\beta}_0^{\sf T}\mathbf{Z}_j)}\bigr)dN_i(u)
$$
*is a martingale (Section 3.3).*

Each individual counting process can be decomposed as
$$N_i(t)=\int_0^t \alpha_0(u)Y_i(u)\exp(\mathbf{\beta}_0^{\sf T}\mathbf{Z}_i)du + M_i(t)$$
and with $S_0(\mathbf{\beta},t)=\sum_iY_i(t)\exp(\mathbf{\beta}^{\sf T}\mathbf{Z}_i)$, $\mathbf{S}_1(\mathbf{\beta},t)=\sum_iY_i(t)\mathbf{Z}_i\exp(\mathbf{\beta}^{\sf T}\mathbf{Z}_i)$, the integrand in the Cox score becomes
$$\sum_i \bigl(\mathbf{Z}_i-\frac{\mathbf{S}_1(\mathbf{\beta},u)}{S_0(\mathbf{\beta},u)}\bigr)(\alpha_0(u)Y_i(u)\exp(\mathbf{\beta}_0^{\sf T}\mathbf{Z}_i)du + dM_i(u))
$$
$$=\alpha_0(u)\bigl(\mathbf{S}_1(\mathbf{\beta},u)-\frac{\mathbf{S}_1(\mathbf{\beta},u)}{{S}_0(\mathbf{\beta},u)}{S}_0({\beta},u)\bigr)du
+ \sum_i(\mathbf{Z}_i-\frac{\mathbf{S}_1(\mathbf{\beta},u)}{S_0(\mathbf{\beta},u)}\bigr)dM_i(u)$$
and the Cox score is then the martingale integral
$$\int_0^t \sum_i(\mathbf{Z}_i-\frac{\mathbf{S}_1(\mathbf{\beta},u)}{S_0(\mathbf{\beta},u)}\bigr)dM_i(u).$$

### Exercise 3.3 (\*)
*Show that, for a Cox model with a single binary covariate, the score test for the hypothesis* $\beta=0$ *based on the first and second derivative of* $\log\mbox{PL}(\beta)$ *(Equation (3.16)) is equal to the logrank test.*}

With a single binary covariate $Z_i=I(i\mbox{ is in group 1})$, the Cox score
$$\sum_i \int_0^t\left(Z_i-\frac{S_1(\beta,u)}{S_0(\beta,u)}\right)dN_i(u)$$
(with $S_0, S_1$ defined in Exercise 3.2) reduces to
$$N_1(\infty)-\int_0^{\infty}\frac{Y_1(u)}{Y_0(u)+Y_1(u)}d(N_0+N_1)(u)$$
when $\beta=0$ and $t=\infty$. This is the numerator in the logrank test, $\mbox{LR}(\infty)$, Section 3.2.2. With $S_2(\beta,t)=\sum_jZ_j^2Y_j(t)\exp(\beta Z_j)$, minus the derivative of the Cox score is
$$\sum_i\int_0^t\frac{S_0(\beta,u)S_2(\beta,u)-S_1(\beta,u)^2}{S_0(\beta,u)^2}dN_i(u)$$
which for $\beta=0$ and $t=\infty$ (when there are no ties) reduces to the hypergeometric variance, $v$ (Section 3.2.2)
$$\int_0^{\infty}\frac{Y_0(u)Y_1(u)}{(Y_0(u)+Y_1(u))^2}d(N_0+N_1)(u)$$
and the resulting score test is the logrank test.

### Exercise 3.4 (\*)

*Show that, for the stratified Cox model (3.20), the profile likelihood is given by (3.21) and the resulting Breslow estimator by (3.22).*

The log-likelihood for the stratified Cox model based on the Jacod formula is
$$\sum_j\sum_{i\in S_j}\left(\int_0^{\infty}\log\bigl(Y_i(t)\alpha_{0j}(t)\exp(\mathbf{\beta}^{\sf T}\mathbf{Z}_i)\bigr)dN_i(t)-\int_0^{\infty}Y_i(t)\alpha_{0j}(t)\exp(\mathbf{\beta}^{\sf T}\mathbf{Z}_i)dt\right)$$
(Section 3.3). Taking derivative with respect to a single $\alpha_{0j}(t)$, we get
$$\sum_{i \in S_j}\left(\frac{1}{\alpha_{0j}(t)}dN_i(t)-Y_i(t)\exp(\mathbf{\beta}^{\sf T}\mathbf{Z}_i)dt\right),$$
and equating to 0 and solving leads to
$$\widehat{\alpha_{0j}(t)dt}=\frac{\sum_{i\in S_j}dN_i(t)}{\sum_{i \in S_j}Y_i(t)\exp(\mathbf{\beta}^{\sf T}\mathbf{Z}_i)},$$
the jump in the Breslow estimator (3.22).

Inserting this into the Jacod formula leads to the profile likelihood
$$\prod_j\prod_{i\in S_j}\exp\left(-\int_0^{\infty}Y_i(t)\frac{\sum_{\ell\in S_j}dN_\ell(t)}{\sum_{\ell \in S_j}Y_\ell(t)\exp(\mathbf{\beta}^{\sf T}\mathbf{Z}_\ell)}\exp(\mathbf{\beta}^{\sf T}\mathbf{Z}_i)dt\right)$$
$$\times \prod_j\prod_{i\in S_j}\prod_t \left(Y_i(t)\frac{\sum_{\ell\in S_j}dN_\ell(t)}{\sum_{\ell \in S_j}Y_\ell(t)\exp(\mathbf{\beta}^{\sf T}\mathbf{Z}_\ell)}\exp(\mathbf{\beta}^{\sf T}\mathbf{Z}_i)\right)^{dN_i(t)},$$
where, in the first line, factors depending on $\mathbf{\beta}$ cancel, and the remaining factors depending on $\mathbf{\beta}$ are exactly (3.21).

### Exercise 3.5 (\*)

*Consider the situation in Section 3.4 with categorical covariates and show that the likelihood is given by*
$$
\prod_{\ell=1}^L\prod_{\mathbf{c}\in {\cal C}}(\alpha_{0\ell}\theta_{\mathbf{c}})^{N_{\ell \mathbf{c}}}\exp(-\alpha_{0\ell}\theta_{\mathbf{c}} Y_{\ell \mathbf{c}}).
$$

The starting point is the Jacod formula where factor $i$ is
$$\prod_t (Y_i(t)\alpha_i(t))^{dN_i(t)}\exp(-\int_0^{\infty}Y_i(t)\alpha_i(t)dt),$$
and when $\alpha_i(t)=\alpha_{0\ell}\mathbf{\theta}_{\mathbf{c}}$ if $t\in [s_{\ell -1},s_{\ell})$ and $i$ has covariates in category $\mathbf{c}$, this is proportional to
$$\prod_{\ell=1}^L(\alpha_{0\ell}\mathbf{\theta}_{\mathbf{c}})^{N_{\ell\mathbf{c}}^i}\exp(-\sum_{\ell=1}^LY_{\ell\mathbf{c}}^i\mathbf{\theta}_{\mathbf{c}}),$$
where $N_{\ell\mathbf{c}}^i$ is the number of events for subject $i$ in $[s_{\ell -1},s_{\ell})$ and $Y_{\ell\mathbf{c}}^i$ the time spent by subject $i$ in $[s_{\ell -1},s_{\ell})$. Collecting factors from all subjects with covariates in category $\mathbf{c}$, the joint likelihood contribution from those subjects is
$$\prod_{\ell=1}^L(\alpha_{0\ell}\mathbf{\theta}_{\mathbf{c}})^{N_{\ell\mathbf{c}}}\exp(-\sum_{\ell=1}^LY_{\ell\mathbf{c}}\mathbf{\theta}_{\mathbf{c}}),$$
where $N_{\ell\mathbf{c}}$ and $Y_{\ell\mathbf{c}}$ are the sums over those subjects of the individual values of $N_{\ell\mathbf{c}}^i$ and $Y_{\ell\mathbf{c}}^i$, respectively. The total likelihood is now the product over all categories $\mathbf{c}$.

### Exercise 3.6 (\*)
*Derive the estimating equations for the model studied in Section 3.8.4*

The arguments are, in fact, identical to those used for the stratified Cox model in Exercise 3.4. The only difference appears due to the fact that the likelihood does not factorize over types, $\nu$.
